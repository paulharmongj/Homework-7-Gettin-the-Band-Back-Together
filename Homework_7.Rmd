---
title: 'Homework 7:'
author: "Paul Harmon, Jacob Dym, and Justin Gomez"
date: "October 31, 2016"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=F,message=F,comment=NA,fig.width=6,fig.height=4)
```

1) _We are going to be exploring a more complete model that includes a long-term trend and a seasonal component. We need to pick the type of trend and the form of the seasonal component. For the long term trend, consider the following options: no trend, linear trend, or quadratic trend. For the seasonal component, consider no seasonal component, seasonal means, single harmonic pair (m=1), and 5th order harmonic (m=5). Consider all combinations of these components, fit using `lm`.  Create a table that contains the model description, model used DF (so the count of free parameters), AICs, and $\Delta$AICs, sorting the table by AIcs. Use this information to discuss the top model selected (what was in it), the strength of support for that model versus the others, and the strength of evidence for a long-term trend and seasonal component (of the type selected) versus not including them in the model._ 

  - __You should be creating a table that contains twelve models.__

The table below dispalys information for the twelve models created with different combinations of trends and seasonal components. The table has been sorted by AIC for each model, from lowest to highest, allowing us to essentially rank the models in terms of fit. The lowest AIC is the "best" model. In this case, the best choice is the model with a quadratic trend and single harmonic. We could consider that there is a reasonably large difference between the best AIC-selected model and the next best couple of models; further, the 2nd best involves a 5th-order harmonic, so it may have higher model complexity as well.  The t-test for a long-term trend indicates strong evidence of a long term trend; further, it indicates that the harmonic pieces should be included in the model as well. 

```{r prob1,tidy=TRUE,tidy.opts=list(width.cutoff=50)}
#read in the data
CGO <- read.table("ftp://aftp.cmdl.noaa.gov/data/trace_gases/co2/flask/surface/co2_cgo_surface-flask_1_ccgg_month.txt",header=T)
colnames(CGO) = c("Site", "Year", "Month", "Value")

CGOts<-ts(CGO$Value,start=c(1984,5),freq=12)
YearF <- as.vector(time(CGOts))
MonthF <- as.factor(cycle(CGOts))

library(TSA)

#m1 - no trend, no seasonal component
m1<-lm(CGO$Value~1)
fits1<-fitted(m1)

#m2 - no trend, seasonal means
m2<-lm(CGO$Value~1+MonthF)
fits2<-fitted(m2)

#m3 - no trend, single harm
m3<-lm(CGO$Value~1+harmonic(CGOts,m=1))
fits3<-fitted(m3)

#m4 - no trend, 5th order harm
m4<-lm(CGO$Value~1+harmonic(CGOts,m=5))
fits4<-fitted(m4)

#m5 - linear trend, no season
m5<-lm(CGO$Value~YearF)
fits5<-fitted(m5)

#m6 - linear trend, seasonal means
m6<-lm(CGO$Value~YearF+MonthF)
fits6<-fitted(m6)

#m7 - linear trend, single harm
m7<-lm(CGO$Value~YearF+harmonic(CGOts,m=1))
fits7<-fitted(m7)

#m8 - linear trend, 5th order harm
m8<-lm(CGO$Value~YearF+harmonic(CGOts,m=5))
fits8<-fitted(m8)

#m9 - quad trend, no season
m9<-lm(CGO$Value~poly(YearF,degree=2))
fits9<-fitted(m9)

#m10 - quad trend, seasonal means
m10<-lm(CGO$Value~poly(YearF,degree=2)+MonthF)
fits10<-fitted(m10)

#m11 - quad trend, single order harm
m11<-lm(CGO$Value~poly(YearF,degree=2)+harmonic(CGOts,m=1))
fits11<-fitted(m11)

#m12 - quad trend, 5th order harm
m12<-lm(CGO$Value~poly(YearF,degree=2)+harmonic(CGOts,m=5))
fits12<-fitted(m12)

#real inefficient code...
allthefits<-cbind(fits1,fits2,fits3,fits4,fits5,fits6,fits7,fits8,fits9,fits10,fits11,fits12)
matplot(YearF,allthefits,type="l",col=1:12,lty=1:12)
```


```{r table1,tidy=TRUE,tidy.opts=list(width.cutoff=50)}
library(pander)
alltheAICs<-AIC(m1,m2,m3,m4,m5,m6,m7,m8,m9,m10,m11,m12)
changeinAIC<-rep(12,12)
for(i in 1:12) {
  changeinAIC[i]<-alltheAICs$AIC[i]-min(alltheAICs$AIC)
}
unsorted<-data.frame(cbind(alltheAICs,changeinAIC))
rownames(unsorted)<-c("No trend, no seasonal component","No trend, seasonal means","No trend, single harmonic","No trend, 5th order harmonic","Linear trend, no seasonal component","Linear trend, seasonl means","Linear trend, single harmonic","Linear trend, 5th order trend","Quad trend, no seasonal component","Quad trend, seasonal means","Quad trend, single harmonic","Quad trend, 5th order harmonic")
colnames(unsorted)<-c("Num. of free pars","AIC","Change in AIC")
sorted<-unsorted[order(unsorted$AIC),]
pandoc.table(sorted)
```


2) _Now fit a `gam` from the `mgcv` package that includes a long-term trend based on a thin-plate spline with shrinkage that uses `k=#years,bs="ts"` from the fractional year variable and a cyclic spline seasonal component. To build the cyclic spline component, use the numerically coded month variable that goes from 1 to 12 and `k=12,bs="cc"`. Fit the model, plot the long-term trend and the seasonal component and discuss the estimated components, using both the plots and the EDF of each term._
The GAM is fitted below. The model estimates a long-term trend as well as a monthly (seasonal) component. The estimated degrees of freedom (EDF) are 29.24 for the smoothed (nonparametric) long-term effect and 5.14 for the smoothed seasonal component. This means that the model is estimating a relatively rough curve for the long-term trend because it is using roughly 29 degrees of freedom; the seasonal component has fewer estimated degrees of freedom so it is relatively smooth by comparison. The plots verify this as the monthly curve appears nearly linear in comparison to the much more "wiggly" curve for the long-term trend. 


```{r prob2,tidy=TRUE,tidy.opts=list(width.cutoff=50),fig.height=5.5}
library(mgcv)
monthymcmonthface <- CGO$Month
gam.model<-gam(CGOts~s(time(CGOts),k=31,bs="ts")+s(monthymcmonthface,k=12,bs="cc"))
par(mfrow=c(2,1))
plot(gam.model)
summary(gam.model)
```


3) _Calculate the AIC of the GAM using the `AIC` function and discuss how that result compares to your AICs in #1. How is it similar or different in terms of information (degrees of freedom) used?_
The AIC value for the GAM is given in the output below as -210.76. Note that this is different from the values calculated in the previous table; these values are not comparable with each other because the models being used are not nested. That being said, the GAM uses 29.24 and 5.14 EDF whereas the non-smoothed models use, in general, fewer degrees of freedom to calculate.  
```{r prob3,tidy=TRUE,tidy.opts=list(width.cutoff=50)}
AIC(gam.model)
```


4) _Compare the fitted values of your GAM to those from your top model, plotting the two models's results and the responses vs time on the same plot._

The fitted values of the GAM, the top model, and the responses are all relatively similar. Note that near the edges the top-fitting model appears to differ somewhat from the actual response (CO2); however, the GAM fits very similarly for the entire range of time.  It appears the the GAM performs better than the Top-Fitting Model as it more consistently matches the actual CO2 responses from 1984 to 2016. 

```{r prob4,tidy=TRUE,tidy.opts=list(width.cutoff=50)}
topfit<-fitted(m7)
gamfit<-fitted(gam.model)
par(mfrow=c(1,1))
plot(CGOts,col="hotpink",lwd=4, main = "Comparison of Fitted Values", ylab = "CO2")
lines(topfit~YearF,type="l",col="red",lwd=3)
lines(gamfit~YearF,type="l",col="blue",lwd=3,lty=2)
legend("topleft",fill = c("hotpink","red","blue"), c("CO2","Top Fitting Model","GAM"))
```

## A simulation study with autocorrelation present

5) _Revisit your simulation with an AR(1) from HW 6 \# 10. Consider fitting a model with autocorrelation in it using `gls` from the `nlme` package that accounts for an MA(1) error and another that accounts for an AR(1) error. Run your simulation code, extracting the p-values from the two model summaries and estimate the type I error rate in each situation and compare it to what you get from the regular linear model._ 

- _In a `gls` model summary, the estimates, SEs, test statistics, and p-values are contained in the `tTable` part of the summary. It has a similar layout to `coef` that we pulled the [2,4] element from to get the p-value from `lm`._

The simulation study is given below. Using the MA(1) error from the GLS function and the AR(1) structure, we extracted the p-values from each model summary and estimated the respective Type I error rates. With the GLS function using AR(1) we appear to get a .05 error rate (as expected) and for the MA(1) error, we obtain a Type I error rate of 0.066, which is slightly inflated.  

```{r prob 5,tidy=TRUE,tidy.opts=list(width.cutoff=50),cache=TRUE}
library(nlme)
set.seed(10282016)
num.sims=100
x=seq(1,109,by=1)
happened.ma<-logical(length=num.sims)
happened.ar<-logical(length=num.sims)
happened.lm<-logical(length=num.sims)
DAT<-as.data.frame(cbind(seq(1,109,by=1),rep(12,109)))
colnames(DAT)<-c("index","noise")

for(i in 1:num.sims) {
  DAT[,2]<-arima.sim(n=109,model=list(ar=c(.6)),sd=sqrt(0.0006664659))
  model.gls.ma<-gls(noise~index,data=DAT,correlation=corARMA(p=0,q=1))
  model.gls.ar<-gls(noise~index,data=DAT,correlation=corARMA(p=1,q=0))
  model.lm<-lm(noise~index,data=DAT)
  pvalue.ar<-summary(model.gls.ar)$tTable[2,4]
  pvalue.ma<-summary(model.gls.ma)$tTable[2,4]
  pvalue.lm<-summary(model.lm)$coefficient[2,4]
  happened.ma[i]<-pvalue.ma<.05
  happened.ar[i]<-pvalue.ar<.05
  happened.lm[i]<-pvalue.lm<.05
}
sim.gls.ma<-sum(happened.ma==TRUE)/num.sims
sim.gls.ma
sim.gls.ar<-sum(happened.ar==TRUE)/num.sims
sim.gls.ar
sim.lm<-sum(happened.lm==TRUE)/num.sims
sim.lm
```

## Some derivation practice (these can be handwritten). If you have not completed STAT 421 or equivalent, please try the problem and then take advantage of advanced help by stopping by to chat about your answer. 

6) _Answer Cryer and Chan question 2.4 (page 20)_


a. __Part A is as follows.__ 

In general, the autocorrelation function is calculated as follows:
$$\rho_{t,s} = \frac{\gamma_{t,s]}}{\sqrt(\gamma_{s,s}\gamma_{t,t}}$$ 

This is equal to:
$$\frac{\theta\sigma^2_{w}}{\sqrt((1+\theta^{2})\sigma^{2}_{w})}$$

Cancelling terms out, we obtain:
$$\frac{\theta}{1+\theta^{2}}$$ 

We can then plug in both values for $\theta$. Note that for $3$ we obtain $\frac{3}{10}$ and we obtain the same answer when plugging in $\frac{1}{3}$ as well. Thus our solution is the same for both values of $\theta$. 

b) __Part B:__ You could/could not determine whether your estimate of $\theta$ is reasonable based on the estimate of $\rho_{k}$. The estimate of $\rho_{k}$ is an estimate of the correlation between a time point at time $t$ and the $t+k$ th time point; it tells us when cycles or trends occur. Thus, if $\rho_{12}$ was large we might have reason to expect a yearly trend occuring in the process, assuming other correlations were small. However, the paramter $\theta$ refers more to an amplitude shifter; it tells us how big of an effect the lag-1 observation has on the current observation; therefore, the only correlation $\rho_{1}$ would be used in this process. The ability to estimate paramater $\theta$, based on your correlations between lags, then, does not seem to be particularly valid unless you have a strong lag-1 process. 


7) _Suppose that we are interested in the properties of a local average (linear filter) of two observations from an original time series, $xt$. The new series is $yt=(0.5)*(x{t-1}+xt)$. The mean of $xt$ is 3, the variance of $xt$ is 4, and the correlation between any neighboring $xt$'s is 0.5 (so $cor(xt, x{t-1})=0.5$). $xt$'s more than two time points apart are uncorrelated (correlation is 0). Use the rules for means and variances of linear combinations to find $E(yt)$, $Var(yt)$, and $Cov(yt,y{t-1})$. Do not worry about what happens at the edges of the time series (for t=1 or t=n), only worry about $t$ in general._

  - _Note that you have some preliminary work to complete to go from the provided information to what you need to work on the three derivations requested._
  
Given $E(x_t)=3$, $Var(x_t)=4$, $cor(x_t,x_{t-1})=0.5$, and the correlation of any $x_{t's}$ further than two time points apart are uncorrelated, we can write the expected value of $y_t$ as;
$$E(y_t)= E(\frac{x_{t-1} + x_t}{2}) = E(\frac{x_{t-1}}{2}) + E(\frac{x_{t}}{2}) = \frac{3}{2} + \frac{3}{2} = 3$$

The variance of y can be calculated similarly; 
$$Var(y_t)= (\frac{1}{2})^{2}Var(x_t) + (\frac{1}{2})^{2}Var(x_{t-1}) = \frac{4}{4} + \frac{4}{4} = 2$$

The covariance of $y_t$ is;
$$Cov(y_t,y_{t-1})= Cov(\frac{x_t + x_{t-1}}{2}, \frac{x_{t-1} + x_{t-2}}{2})= \frac{1}{4}Cov(x_t, x_{t-1}) + \frac{1}{4}Cov(x_{t-1}, x_{t-1}) + \frac{1}{4}Cov(x_t, x_{t-2}) + \frac{1}{4}Cov(x_{t-1}, x_{t-2})= \frac{1}{2} + 1 + 0 + \frac{1}{2}= 2$$


##R Version Information
```{r}
getRversion()
sessionInfo()$R.version$nickname
```


